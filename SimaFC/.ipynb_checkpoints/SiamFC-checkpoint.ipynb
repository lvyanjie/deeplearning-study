{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法总体框架如下图所示：  \n",
    "![structure.png](structure.png)  \n",
    "其中，z代表模板图像，算法采用的是第一帧的groundtruth；x为search region，表示后面待跟踪帧中的候选框搜索区域；$\\phi$表示一种特征映射操作，将原始图像映射到特定的特征空间，论文采用的是CNN中的卷积层和pooling层；6x6x128表示z经过$\\phi$后得到的特征，同理，22x22x128是x经过$\\phi$后得到的特征；图中的 * 代表卷积操作，将6x6x128作为卷积核对22x22x128进行卷积操作，最终得到17x17的score map，表示search region中各个位置和模板相似度值。上图中的$\\phi$结构是一样的，是孪生网络结构，且整个模型只有conv层和pooling层，是一种全卷积网络结构。  \n",
    "算法本身通过比较搜索区域和目标模板的相似度，最后得到搜索区域的score map。然后在score map中找到相似度最大的点，作为新的目标的中心，原理上类似于相关滤波方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、实验目的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 结合论文Dataset curation部分，理解作者对数据的处理方式\n",
    "2. 理解孪生网络（Siamese）结构\n",
    "3. 结合论文Training with large search images理解loss设计以及ground truth的生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、实验步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1. 解析ILSVRC2015 VID数据，并对数据进行处理 (Dataset curation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import xml.etree.cElementTree as ET\n",
    "except ImportError:\n",
    "    import xml.etree.ElementTree as ET\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def sub_process(root_dir, save_dir, split, subdir='', ):\n",
    "    '''\n",
    "    数据处理子进程\n",
    "    root_dir:  数据总路径\n",
    "    save_dir:  处理数据存储路径\n",
    "    split:     包含 snippets，test，train，val\n",
    "    subdir:    相应文件路径下的folders，当split='val'时，subdir=''\n",
    "    '''\n",
    "    data_dir = os.path.join(root_dir, 'Data', 'VID', split)#训练视频文件（以图像格式存储）\n",
    "    anno_dir = os.path.join(root_dir, 'Annotations', 'VID', split, subdir)#object标注文件\n",
    "    video_names = os.listdir(anno_dir)#视频文件名称\n",
    "    \n",
    "    for idx, video in enumerate(video_names):#显示进度条\n",
    "        video_anno_path = os.path.join(anno_dir, video)\n",
    "        xml_files = glob.glob(os.path.join(video_anno_path, '*.xml'))#xml标注文件,每个视频基于帧数有多个标注文件\n",
    "        \n",
    "        #解析xml文件\n",
    "        for xml in xml_files:\n",
    "            tree = ET.parse(xml)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            folder = root.find('folder').text\n",
    "            filename = root.find('filename').text\n",
    "            \n",
    "            #读取图像数据\n",
    "            img_file = os.path.join(data_dir, folder, filename + '.JPEG')\n",
    "            img = cv2.imread(img_file)\n",
    "            \n",
    "            #获取所有object的bounding box\n",
    "            bboxes = []\n",
    "            for object in root.iter('object'):\n",
    "                bbox = object.find('bndbox')\n",
    "                xmax = float(bbox.find('xmax').text)\n",
    "                xmin = float(bbox.find('xmin').text)\n",
    "                ymax = float(bbox.find('ymax').text)\n",
    "                ymin = float(bbox.find('ymin').text)\n",
    "                \n",
    "                width = xmax - xmin + 1\n",
    "                height = ymax - ymin + 1\n",
    "                \n",
    "                bboxes.append([xmin, ymin, width, height])#xmin, ymin, width, height\n",
    "            \n",
    "            for idx, object in enumerate(root.iter('object')):\n",
    "                id = object.find('trackid').text#追踪目标，需要确认，因为一张图可能存在多个追踪目标\n",
    "                class_name = object.find('name').text #追踪目标类别，在追踪场景中，可以忽略\n",
    "                \n",
    "                #创建文件存储路径\n",
    "                track_save_dir = get_track_save_directory(save_dir, split, subdir, video)\n",
    "                if not os.path.exists(track_save_dir):\n",
    "                    os.makedirs(track_save_dir)#创建多层路径\n",
    "                savexfile = os.path.join(track_save_dir, '{}.{:02d}.crop.x.jpg'.format(filename, int(id)))\n",
    "                savezfile = os.path.join(track_save_dir, '{}.{:02d}.crop.z.jpg'.format(filename, int(id)))\n",
    "                #跳过已经处理的文件\n",
    "                if os.path.isfile(savexfile) and os.path.isfile(savezfile):\n",
    "                    continue\n",
    "                \n",
    "                #获取最终的搜索图像数据\n",
    "                target_box = convert_bbox_format(bboxes[idx])\n",
    "                image_crop_z, pad_z, bbox_z, image_crop_x, pad_x, bbox_x = get_crops(img, target_box, size_z=127, size_x=255, context_amount=0.5)\n",
    "                \n",
    "                cv2.imwrite(savexfile, image_crop_x, [int(cv2.IMWRITE_JPEG_QUALITY), 90])#保存 xcrop\n",
    "                cv2.imwrite(savezfile, image_crop_z, [int(cv2.IMWRITE_JPEG_QUALITY), 90])#保存 zcrop\n",
    "    print('finished')\n",
    "\n",
    "def get_track_save_directory(save_dir, split, subdir, video):\n",
    "    '''\n",
    "    返回存储路径，分别存储在a,b,c,d,e文件夹，其中，a,b,c,d为train文件, e为val文件\n",
    "    save_dir: 处理文件存储路径\n",
    "    split:    train or val\n",
    "    subdir:   train文件夹下的subfolder\n",
    "    video:    相应的视频文件路径\n",
    "    '''\n",
    "    subdir_map = {'ILSVRC2015_VID_train_0000': 'a',\n",
    "                'ILSVRC2015_VID_train_0001': 'b',\n",
    "                'ILSVRC2015_VID_train_0002': 'c',\n",
    "                'ILSVRC2015_VID_train_0003': 'd',\n",
    "                '': 'e'}\n",
    "    \n",
    "    return os.path.join(save_dir, 'Data', 'VID', 'train', subdir_map[subdir], video) \n",
    "\n",
    "def convert_bbox_format(bbox):\n",
    "    '''\n",
    "    将原始的bbox转化为center_x, center_y, width, height格式\n",
    "    bbox: list, [xmin, ymin, width, height]\n",
    "    \n",
    "    返回：\n",
    "       bbox: list, [center_x, center_y, width, height]\n",
    "    '''\n",
    "    xmin, ymin, width, height = bbox\n",
    "    c_x = xmin + get_center(width)\n",
    "    c_y = ymin + get_center(height)\n",
    "    return [c_x, c_y, width, height]\n",
    "\n",
    "def get_center(x):\n",
    "    '''\n",
    "    获取x的中点\n",
    "    '''\n",
    "    return (x-1.)/2.\n",
    "\n",
    "def get_crops(img, bbox, size_z, size_x, context_amount):\n",
    "    '''\n",
    "    从原始图像数据中获取搜索图像（子图）,并根据论文Dataset curation部分进行padding处理\n",
    "    img: 原始帧图像数据\n",
    "    bbox: object bbox\n",
    "    size_z: 模板图像size    论文采用127\n",
    "    size_x: 搜索图像size    论文采用255\n",
    "    context_amount: 0.5   对应论文公式参数 2p = 0.5*(w+h)\n",
    "    '''\n",
    "    c_x, c_y, width, height = bbox\n",
    "    wc_z = width + context_amount * (width + height)   #对应公式 w+2p\n",
    "    hc_z = height + context_amount * (width + height)  #对应公式 h+2p\n",
    "    s_z = np.sqrt(wc_z * hc_z)    #未进行scale前的图像size\n",
    "    scale_z = size_z / s_z  #需要进行的缩放因子，最终达到定值127\n",
    "    #获取crop_z\n",
    "    image_crop_z, left_pad_z, top_pad_z, right_pad_z, bottom_pad_z = get_subwindow_avg(img, [c_x, c_y],\n",
    "                                               [size_z, size_z],\n",
    "                                               [np.round(s_z), np.round(s_z)])#image_crop_x: 255 x 255\n",
    "    pad_z = np.ceil([scale_z*(left_pad_z), scale_z*(top_pad_z), scale_z*(right_pad_z), scale_z*(bottom_pad_z)])\n",
    "    \n",
    "    d_search = (size_x - size_z) / 2.#scale之后的图，搜索图需要在模板图的基础上添加多少背景信息\n",
    "    pad = d_search / scale_z         #映射到scale之前的图像大小\n",
    "    s_x = s_z + 2 * pad              #缩放前搜索图像的size大小\n",
    "    scale_x = size_x / s_x           #缩放因子，其实和scale_x基本一致，有精度差别\n",
    "    \n",
    "    image_crop_x, left_pad_x, top_pad_x, right_pad_x, bottom_pad_x = get_subwindow_avg(img, [c_x, c_y],\n",
    "                                               [size_x, size_x],\n",
    "                                               [np.round(s_x), np.round(s_x)])#image_crop_x: 255 x 255\n",
    "    pad_x = np.ceil([scale_x*(left_pad_x), scale_x*(top_pad_x), scale_x*(right_pad_x), scale_x*(bottom_pad_x)])\n",
    "    \n",
    "    #计算bbox_z, bbox_x\n",
    "    ws_z = width * scale_z\n",
    "    hs_z = height * scale_z\n",
    "    ws_x = width * scale_x\n",
    "    hs_x = height * scale_x\n",
    "    bbox_z = [(size_z - ws_z)/2, (size_z - hs_z)/2, ws_z, hs_z]\n",
    "    bbox_x = [(size_x - ws_x)/2, (size_x - hs_x)/2, ws_x, hs_x]\n",
    "    \n",
    "    return image_crop_z, pad_z, bbox_z, image_crop_x, pad_x, bbox_x\n",
    "\n",
    "def get_subwindow_avg(img, pos, model_sz, original_sz):\n",
    "    '''\n",
    "    从原始图像获取original_sz的子图，并进行resize，得到model_sz大小的子图，原图不足\n",
    "    于original_sz大小的分别用各通道的均值进行padding\n",
    "    img:         原始图像   BGR\n",
    "    pos:         object中心坐标\n",
    "    model_sz:    模型要求输入，论文采用255*255\n",
    "    original_sz: crop后得到的原始图像大小\n",
    "    '''\n",
    "    \n",
    "    avg_chans = [np.mean(img[:, :, 0]), np.mean(img[:, :, 1]), np.mean(img[:, :, 2])]#B，G，R均值，用于后面进行padding\n",
    "    if not original_sz:\n",
    "        original_sz = model_sz\n",
    "    im_sz = img.shape    #原始图像大小\n",
    "    assert im_sz[0] > 2 and im_sz[1] > 2   #保证图像不能过小\n",
    "    c = [get_center(s) for s in original_sz] #对应w/2   h/2\n",
    "    \n",
    "    #检查bounding box是否超过原图边界\n",
    "    context_xmin = np.int(np.round(pos[0] - c[0]))   #xmin\n",
    "    context_xmax = np.int(context_xmin + original_sz[0] - 1)#xmax\n",
    "    context_ymin = np.int(np.round(pos[1] - c[1]))   #ymin\n",
    "    context_ymax = np.int(context_ymin + original_sz[1] - 1) #ymax\n",
    "    \n",
    "    #计算是否需要进行padding\n",
    "    left_pad = np.int(np.maximum(0, -context_xmin))#如果xmin是负值，则需要进行left padding\n",
    "    top_pad = np.int(np.maximum(0, -context_ymin))#如果ymin是负值，则需要进行top padding\n",
    "    right_pad = np.int(np.maximum(0, context_xmax - im_sz[1] + 1))#如果xmax超过原图width，则需要进行right padding\n",
    "    bottom_pad = np.int(np.maximum(0, context_ymax - im_sz[0] + 1))#如果ymax超过原图height, 则需要进行bottom padding\n",
    "    \n",
    "    context_xmin = context_xmin + left_pad\n",
    "    context_xmax = context_xmax + left_pad\n",
    "    context_ymin = context_ymin + top_pad\n",
    "    context_ymax = context_ymax + top_pad\n",
    "    \n",
    "    #基于计算出的padding元素对原图进行padding\n",
    "    if top_pad > 0 or bottom_pad > 0 or left_pad > 0 or right_pad > 0:\n",
    "        B = np.pad(img[:, :, 0], ((top_pad, bottom_pad), (left_pad, right_pad)),\n",
    "                   'constant', constant_values=(avg_chans[0]))\n",
    "        G = np.pad(img[:, :, 1], ((top_pad, bottom_pad), (left_pad, right_pad)),\n",
    "                   'constant', constant_values=(avg_chans[1]))\n",
    "        R = np.pad(img[:, :, 2], ((top_pad, bottom_pad), (left_pad, right_pad)),\n",
    "                   'constant', constant_values=(avg_chans[2]))\n",
    "        img = np.stack((B, G, R), axis=2)\n",
    "        \n",
    "    \n",
    "    im_patch_original = img[context_ymin:context_ymax + 1,context_xmin:context_xmax + 1, :]\n",
    "    \n",
    "    if not (model_sz[0] == original_sz[0] and model_sz[1] == original_sz[1]):\n",
    "        im_patch = cv2.resize(im_patch_original, tuple(model_sz))\n",
    "    else:\n",
    "        im_patch = im_patch_original\n",
    "    return im_patch, left_pad, top_pad, right_pad, bottom_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "finished\n",
      "finished\n",
      "finished\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "vid_dir = './ILSVRC2015'\n",
    "save_dir = './ILSVRC2015-VID-Curation'\n",
    "\n",
    "one_work = lambda a, b: sub_process(vid_dir, save_dir, a, b)\n",
    "\n",
    "one_work('train','ILSVRC2015_VID_train_0000')\n",
    "one_work('train','ILSVRC2015_VID_train_0001')\n",
    "one_work('train','ILSVRC2015_VID_train_0002')\n",
    "one_work('train','ILSVRC2015_VID_train_0003')\n",
    "one_work('val', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2. 构建SiameseFC模型 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 构建 feature extractor---AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras import initializers, regularizers, layers, optimizers, callbacks\n",
    "from keras.engine.topology import Layer\n",
    "import keras.backend as K\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "class Split(Layer):\n",
    "    '''\n",
    "    定义split层\n",
    "    '''\n",
    "    def __init__(self, num_or_size_splits =2, axis = 3, **kwargs):\n",
    "        '''\n",
    "        num_or_size_splits: 将feature map沿axis通道分割成几部分，x的维度必须能够整除num_or_size_splits，否则报错\n",
    "        axis: 分割的通道\n",
    "        '''\n",
    "        self.num_or_size_splits = num_or_size_splits\n",
    "        self.axis = axis\n",
    "        super(Split, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, x):\n",
    "        b1, b2 = tf.split(x, self.num_or_size_splits, self.axis)\n",
    "        return [b1, b2]\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "\n",
    "        new_shape = list(input_shape)\n",
    "        new_shape[self.axis] = int(input_shape[self.axis] / self.num_or_size_splits)\n",
    "        new_shape = tuple(new_shape)\n",
    "        \n",
    "        result = []\n",
    "        for i in range(self.num_or_size_splits):\n",
    "            result.append(new_shape)\n",
    "        return result\n",
    "\n",
    "def alexnet(input_shape):\n",
    "    \n",
    "    input_layer = layers.Input(shape = input_shape, name = 'input_layer')\n",
    "    #注意：alexnet论文直接采用11x11的卷积操作，strides = 4\n",
    "    # 此处通过结合conv2d strides=2 和maxpooling2d strides=2 进行\n",
    "    net = layers.Conv2D(filters=96, kernel_size=(11,11), strides=2, \n",
    "                        kernel_initializer = initializers.VarianceScaling(scale=2.0, mode='fan_out'), \n",
    "                              kernel_regularizer = regularizers.l2(5e-4),\n",
    "                              name='conv1')(input_layer)\n",
    "    net = layers.BatchNormalization(momentum = 0.95, epsilon = 1e-06)(net)\n",
    "    net = layers.Activation('relu')(net)\n",
    "    net = layers.MaxPooling2D(pool_size=(3,3), strides=2, name = 'pool1')(net)\n",
    "    \n",
    "    b1, b2 = Split(num_or_size_splits = 2, axis=3, name = 'split1')(net)\n",
    "    b1 = layers.Conv2D(filters = 128, kernel_size=(5, 5), \n",
    "                             kernel_initializer = initializers.VarianceScaling(scale=2.0, mode='fan_out'),\n",
    "                             kernel_regularizer = regularizers.l2(5e-4),\n",
    "                             name = 'conv2_b1')(b1)\n",
    "    b2 = layers.Conv2D(filters = 128, kernel_size=(5, 5), \n",
    "                             kernel_initializer = initializers.VarianceScaling(scale=2.0, mode='fan_out'),\n",
    "                             kernel_regularizer = regularizers.l2(5e-4),\n",
    "                             name = 'conv2_b2')(b2)\n",
    "    net = layers.Concatenate(axis=3)([b1, b2])\n",
    "    net = layers.BatchNormalization(momentum=0.95, epsilon=1e-06)(net)\n",
    "    net = layers.Activation('relu')(net)\n",
    "    net = layers.MaxPooling2D(pool_size=(3,3), strides=2, name='pool2')(net)\n",
    "\n",
    "    net = layers.Conv2D(filters = 384, kernel_size = (3,3), strides = 1, \n",
    "                        kernel_initializer = initializers.VarianceScaling(scale=2.0, mode='fan_out'),\n",
    "                        kernel_regularizer = regularizers.l2(5e-4),\n",
    "                        name = 'conv3')(net)\n",
    "    net = layers.BatchNormalization(momentum = 0.95, epsilon = 1e-06)(net)\n",
    "    net = layers.Activation('relu')(net)\n",
    "    \n",
    "    b1, b2 = Split(num_or_size_splits = 2, axis = 3, name = 'split2')(net)\n",
    "    b1 = layers.Conv2D(filters = 192, kernel_size = (3,3), strides = 1, \n",
    "                             kernel_initializer = initializers.VarianceScaling(scale=2.0, mode='fan_out'),\n",
    "                             kernel_regularizer = regularizers.l2(5e-4),\n",
    "                             name = 'conv4_b1')(b1)\n",
    "    b2 = layers.Conv2D(filters = 192, kernel_size = (3,3), strides = 1, \n",
    "                             kernel_initializer = initializers.VarianceScaling(scale=2.0, mode='fan_out'),\n",
    "                             kernel_regularizer = regularizers.l2(5e-4),\n",
    "                             name = 'conv4_b2')(b2)\n",
    "    net = layers.Concatenate(axis=3)([b1, b2])\n",
    "    net = layers.BatchNormalization(momentum=0.95, epsilon=1e-06)(net)\n",
    "    net = layers.Activation('relu')(net)\n",
    "    \n",
    "    b1, b2 = Split(num_or_size_splits = 2, axis = 3, name = 'split3')(net)\n",
    "    b1 = keras.layers.Conv2D(filters = 128, kernel_size = (3,3), strides = 1, \n",
    "                             kernel_initializer = initializers.VarianceScaling(scale=2.0, mode='fan_out'),\n",
    "                             kernel_regularizer = regularizers.l2(5e-4),\n",
    "                             name = 'conv5_b1')(b1)\n",
    "    b2 = keras.layers.Conv2D(filters = 128, kernel_size = (3,3), strides = 1, \n",
    "                             kernel_initializer = initializers.VarianceScaling(scale=2.0, mode='fan_out'),\n",
    "                             kernel_regularizer = regularizers.l2(5e-4),\n",
    "                             name = 'conv5_b2')(b2)\n",
    "    net = keras.layers.Concatenate(axis=3)([b1, b2])\n",
    "    \n",
    "    model = keras.models.Model(inputs = input_layer, outputs = net)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 构建Siamese孪生网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Siamese(template_size, instance_size):\n",
    "\n",
    "    def detection(inputs):\n",
    "        '''\n",
    "        将template_feature和instance_feature进行卷积计算，并在此基础上+bias\n",
    "        获得1x17x17x1的score map\n",
    "        '''\n",
    "        f_x, f_z = inputs\n",
    "        f_x = f_x[0]\n",
    "        f_z = f_z[0]\n",
    "        f_x = K.expand_dims(f_x, 0)\n",
    "        f_z = K.expand_dims(f_z, -1)\n",
    "\n",
    "        score_map = K.conv2d(f_x, f_z, strides=(1, 1), padding='valid')\n",
    "        score_map = score_map[0:3]#删除最后一维\n",
    "        bias = K.variable(value=[0], dtype=K.floatx(), name='detction_bias')\n",
    "        detection = K.bias_add(0.001*score_map, bias)\n",
    "        #尝试采用sigmoid激活  激活函数已经包含sigmoid激活步骤\n",
    "        #detection = K.sigmoid(detection)#对detection结果进行sigmoid激活，将输出值约束到0-1之间\n",
    "        return detection  #17x17\n",
    "    \n",
    "    base_model = alexnet(input_shape = (None, None, 3))#接收任意尺寸   \n",
    "     \n",
    "    template_input = layers.Input(shape = (template_size, template_size, 3))\n",
    "    instance_input = layers.Input(shape = (instance_size, instance_size, 3))\n",
    "    \n",
    "    feature_template = base_model(template_input)       #(?, 6, 6, 256)\n",
    "    feature_instance = base_model(instance_input)       #(?, 22, 22, 256)\n",
    "\n",
    "    #不支持batch_size>1的情况\n",
    "    score_map = layers.Lambda(detection, name='detection')([feature_instance, feature_template])\n",
    " \n",
    "    siamese_model = keras.models.Model(inputs = [template_input, instance_input], outputs = score_map)\n",
    "    \n",
    "    return siamese_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3. 构建ground truth **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gt_construct(target_size = 17, stride = 8, config_rPos = 16, config_rNeg = 0):\n",
    "    y = np.array(list(range(0, target_size))) - (target_size-1)/2\n",
    "    x = np.array(list(range(0, target_size))) - (target_size-1)/2\n",
    "    [Y,X] = np.meshgrid(y, x)\n",
    "\n",
    "    def _logistic_label(X, Y, rPos, rNeg):\n",
    "        dist_to_center = np.abs(X) + np.abs(Y)\n",
    "        Z = np.where(dist_to_center <= rPos,\n",
    "                    np.ones_like(X),\n",
    "                    np.where(dist_to_center < rNeg,\n",
    "                            0.5 * np.ones_like(X),\n",
    "                            np.zeros_like(X)))\n",
    "        return Z\n",
    "    \n",
    "    rPos = config_rPos / stride\n",
    "    rNeg = config_rNeg / stride\n",
    "    gt = _logistic_label(X, Y, rPos, rNeg)\n",
    "    \n",
    "    return gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 4. 构建train_generator和val_generator **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据预处理\n",
    "def preprocess(img_data):\n",
    "    '''\n",
    "    对图像数据进行归一化处理\n",
    "    '''\n",
    "    img_data = img_data / 255  \n",
    "    return img_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 train generator构建，对数据采用随机抽样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(files, batch_size = 1):\n",
    "    '''\n",
    "    迭代器\n",
    "    采用抽样的方式\n",
    "    后期考虑：抽样过程是否需要考虑track_id的平衡\n",
    "    '''\n",
    "    while True:\n",
    "        #数据随机打乱\n",
    "        random.shuffle(files)\n",
    "        \n",
    "        trainx = []\n",
    "        trainz = []\n",
    "        trainY = []\n",
    "        batch_files = files[0:batch_size]\n",
    "        for file in batch_files:\n",
    "            #file是*.crop.x.jpg\n",
    "            crop_x_file = file\n",
    "            crop_z_file = file.replace('.x.jpg', '.z.jpg')\n",
    "            \n",
    "            img_crop_x = cv2.imread(crop_x_file)\n",
    "            img_crop_z = cv2.imread(crop_z_file)\n",
    "\n",
    "            crop_x_input = preprocess(img_crop_x)#归一化处理\n",
    "            crop_z_input = preprocess(img_crop_z)\n",
    "\n",
    "            #\n",
    "            trainx.append(crop_x_input)\n",
    "            trainz.append(crop_z_input)\n",
    "            \n",
    "            #构建gt\n",
    "            trainY.append(gt_construct())#采用默认参数\n",
    "        \n",
    "        trainx = np.array(trainx)\n",
    "        trainz = np.array(trainz)\n",
    "        trainY = np.array(trainY)\n",
    "\n",
    "        #expand_dims\n",
    "        trainY = np.expand_dims(trainY, axis=3)\n",
    "        \n",
    "        yield [trainz, trainx], trainY  #训练用X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 val generator,直接遍历所有validation数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_data_generator(files, batch_size = 1):\n",
    "    '''\n",
    "    验证集validation\n",
    "    验证集不需要进行随机抽样\n",
    "    '''\n",
    "    while True:\n",
    "        batch_num = int(len(files)/batch_size)\n",
    "        for i in range(batch_num):\n",
    "            batch_files = files[i*batch_size: (i+1)*batch_size]\n",
    "            trainx = []\n",
    "            trainz = []\n",
    "            trainY = []\n",
    "            for file in batch_files:\n",
    "                crop_x_file = file\n",
    "                crop_z_file = file.replace('.x.jpg','.z.jpg')\n",
    "\n",
    "                img_crop_x = cv2.imread(crop_x_file)\n",
    "                img_crop_z = cv2.imread(crop_z_file)\n",
    "\n",
    "                crop_x_input = preprocess(img_crop_x)\n",
    "                crop_z_input = preprocess(img_crop_z)\n",
    "\n",
    "                trainx.append(crop_x_input)\n",
    "                trainz.append(crop_z_input)\n",
    "\n",
    "                trainY.append(gt_construct())\n",
    "            \n",
    "            trainx = np.array(trainx)\n",
    "            trainz = np.array(trainz)\n",
    "            trainY = np.array(trainY)\n",
    "\n",
    "            trainY = np.expand_dims(trainY, axis=3)\n",
    "            yield[trainz, trainx], trainY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 5. 构建loss函数 ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_loss(y_true, y_pred):\n",
    "    '''\n",
    "    y_true: shape is 1x17x17x1\n",
    "    y_pred: shape is 1x17x17x1\n",
    "    '''\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_pred,labels = y_true)\n",
    "    \n",
    "    #获取class_weights\n",
    "    n_pos = tf.reduce_sum(tf.to_float(tf.equal(y_true[0,:,:,0], 1)))\n",
    "    n_neg = tf.reduce_sum(tf.to_float(tf.equal(y_true[0,:,:,0], 0)))\n",
    "    w_pos = 0.5 / n_pos\n",
    "    w_neg = 0.5 / n_neg\n",
    "    class_weights = tf.where(tf.equal(y_true, 0),\n",
    "                            w_pos * tf.ones_like(y_true),\n",
    "                            tf.ones_like(y_true))\n",
    "    class_weights = tf.where(tf.equal(y_true, 0),\n",
    "                            w_neg * tf.ones_like(y_true),\n",
    "                            class_weights)\n",
    "    \n",
    "    loss = loss * class_weights\n",
    "    loss = tf.reduce_mean(tf.reduce_sum(loss, [1, 2]))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 6. 模型训练 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5000/5000 [==============================] - 12989s 3s/step - loss: 1.7417 - val_loss: 5.7067\n",
      "Epoch 2/100\n",
      "5000/5000 [==============================] - 13910s 3s/step - loss: 1.5996 - val_loss: 3.9254\n",
      "Epoch 3/100\n",
      "5000/5000 [==============================] - 12379s 2s/step - loss: 1.5491 - val_loss: 3.3657\n",
      "Epoch 4/100\n",
      "5000/5000 [==============================] - 11790s 2s/step - loss: 1.5129 - val_loss: 4.1190\n",
      "Epoch 5/100\n",
      "5000/5000 [==============================] - 14317s 3s/step - loss: 1.4856 - val_loss: 4.6613\n",
      "Epoch 6/100\n",
      "5000/5000 [==============================] - 15113s 3s/step - loss: 1.4601 - val_loss: 13.6532\n",
      "Epoch 7/100\n",
      " 390/5000 [=>............................] - ETA: 2:20:13 - loss: 1.4555"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train_all_crop_img_files = []\n",
    "val_all_crop_img_files = []\n",
    "\n",
    "train_folder_path = './ILSVRC2015-VID-Curation/Data/VID/train'   #for test\n",
    "train_folders = ['a','b','c','d']\n",
    "for folder in train_folders:\n",
    "    video_ids = os.listdir(os.path.join(train_folder_path, folder))\n",
    "    for video_id in video_ids:\n",
    "        train_all_crop_img_files = train_all_crop_img_files + glob.glob(os.path.join(train_folder_path, folder, video_id, '*.crop.x.jpg'))#获取训练数据文件\n",
    "\n",
    "val_folder_path = './ILSVRC2015-VID-Curation/Data/VID/train/e'\n",
    "video_ids = os.listdir(val_folder_path)\n",
    "for video_id in video_ids:\n",
    "    val_all_crop_img_files = val_all_crop_img_files + glob.glob(os.path.join(val_folder_path, video_id, '*.crop.x.jpg'))#获取验证数据文件\n",
    "\n",
    "#受网络框架限制，只接受batch_size=1的情况    \n",
    "train_gen = data_generator(train_all_crop_img_files, batch_size=1)\n",
    "val_gen = val_data_generator(val_all_crop_img_files, batch_size=1)\n",
    "\n",
    "model = Siamese(template_size=127, instance_size=255)\n",
    "\n",
    "opt = optimizers.SGD(lr=0.001)\n",
    "model.compile(optimizer=opt, loss=weighted_loss)\n",
    "\n",
    "#model fit参数\n",
    "modelcheckpoint = callbacks.ModelCheckpoint('simase_fc.h5')\n",
    "tensorboard = callbacks.TensorBoard('./logs')\n",
    "\n",
    "model.fit_generator(generator=train_gen, \n",
    "                    steps_per_epoch = 5000, \n",
    "                    epochs=100,\n",
    "                    callbacks = [modelcheckpoint, tensorboard],\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=len(val_all_crop_img_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、实验注意事项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 实验中，由template_feature$(\\phi(z))$和instance_feature$(\\phi(x))$卷积得到的output，在乘以0.001之后再加bias，论文中未体现此trick。    \n",
    "2. 在构建detection过程中，忽略了feature map的batch大小，因此，模型只接受batch_size=1的情况。  \n",
    "3. 处理后的template和instance图像数据，其追踪目标都位于图像中心位置，基于此前提条件，构建了样本的ground truth。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、实验拓展"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 尝试在理解注意事项1的基础上，将score_map的输出公式变为$f(z,x)=\\omega\\times(\\phi(z)*\\phi(x)) + b$，w和b均可训练模式，查看loss变换情况。 \n",
    "2. 尝试对模型进行改进，能够接受batch_size>1的情况。  \n",
    "3. 实验未集成数据增强方法，尝试优化代码，集成数据增强方法，以增强模型的泛化性能。  \n",
    "4. 实验未集成基于训练好的model进行track，尝试自己加以完善。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
