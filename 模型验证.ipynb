{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_table('../VOCdevkit/VOC2012/ImageSets/Main/car_train.txt', delim_whitespace=True, names=('file_name', 'label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#解析pascal voc数据\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "label_files_path = '../VOCdevkit/VOC2012/ImageSets/Main/*.txt' \n",
    "files = glob.glob(label_files_path)\n",
    "image_files_path = '../VOCdevkit/VOC2012/JPEGImages/' \n",
    "\n",
    "#使用字典存储每个类的文件路径\n",
    "\n",
    "#data_train key为label，value为文件名称，考虑到类别平衡问题\n",
    "data_train = {}\n",
    "#data_val key为文件名称，value为label，方便处理\n",
    "data_val = {}\n",
    "val_files = []#用于存储所有的文件\n",
    "\n",
    "#获取label，并初始化字典\n",
    "for file in files:\n",
    "    if '_train.txt' in file:#\n",
    "        label = file.split('/')[-1].split('.txt')[0].split('_')[0]\n",
    "        data_train[label] = []\n",
    "\n",
    "keys = data_train.keys()\n",
    "data_label = {}\n",
    "i = 0\n",
    "for key in keys:\n",
    "    data_label[key] = i\n",
    "    i+=1\n",
    "\n",
    "#获取每个类对应的数据文件\n",
    "for file in files:\n",
    "    if '_train.txt' in file:\n",
    "        label = file.split('/')[-1].split('.txt')[0].split('_')[0]\n",
    "        df = pd.read_table(file, delim_whitespace=True, names=('file_name', 'label'))\n",
    "        #获取df中label==1的file_name\n",
    "        df_positive = df[df['label']==1]#获取对应的正类文件\n",
    "        for index, row in df_positive.iterrows():\n",
    "            data_train[label].append(image_files_path + row['file_name'] + '.jpg')\n",
    "    if '_val.txt' in file:\n",
    "        label = file.split('/')[-1].split('.txt')[0].split('_')[0]#value\n",
    "        df = pd.read_table(file, delim_whitespace=True, names=('file_name', 'label'))\n",
    "        #获取df中label==1的file_name\n",
    "        df_positive = df[df['label']==1]#获取对应的正类文件, 对应的文件为key\n",
    "        for index, row in df_positive.iterrows():\n",
    "            val_file_path = image_files_path + row['file_name'] + '.jpg'\n",
    "            val_files.append(val_file_path)\n",
    "            #直接存储label值\n",
    "            data_val[val_file_path] = data_label[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建迭代器，并考虑每个batch每类数据的平衡性\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess(img, target_size = 224):\n",
    "    '''\n",
    "    数据处理，包含两部分：\n",
    "    S1：resize操作\n",
    "    S2: normal操作\n",
    "    '''\n",
    "    img = cv2.resize(img, (target_size, target_size))\n",
    "    \n",
    "    #normal\n",
    "    img = img / 127.5\n",
    "    img = img - 1.\n",
    "    \n",
    "    return img\n",
    "\n",
    "def load_image(batch_files):\n",
    "    '''\n",
    "    图像数据加载\n",
    "    batch_files: 待加载的数据列表\n",
    "    '''\n",
    "    batch_data = []\n",
    "    \n",
    "    for file in batch_files:\n",
    "        img = cv2.imread(file)\n",
    "        img = preprocess(img, target_size=224)\n",
    "        batch_data.append(img)\n",
    "        \n",
    "    batch_data = np.array(batch_data)\n",
    "    return batch_data\n",
    "\n",
    "def train_generator(data, data_label, batch_size=32, steps=20):\n",
    "    '''\n",
    "    每次从数据样本中抽取4类数据进行训练\n",
    "    :param data: dict, 训练数据\n",
    "    :param batch_size: integer， batch大小，最小为4\n",
    "    :param steps: integer， 单次epoch运行次数\n",
    "    '''\n",
    "    while True:\n",
    "        keys = data.keys()\n",
    "        while True:\n",
    "            classes = random.sample(keys, 4)#随机选取其中的四类\n",
    "            \n",
    "            #求取每类样本的数目\n",
    "            batch_num = int(batch_size / 4)\n",
    "            batch_files = []\n",
    "            batch_label = []#保留label值\n",
    "            for cls in classes:\n",
    "                files = data[cls]#获取当前类别的所有文件\n",
    "                random.shuffle(files)#随机打乱\n",
    "                batch_files = batch_files + files[0:batch_num]#获取每一类数据的batch，组成batch_files\n",
    "                batch_label.append(data_label[cls])#保存所取数据的类别，最终生成的类别list需要再次进行repeat 4操作          \n",
    "            \n",
    "            batch_data = load_image(batch_files)\n",
    "            batch_label = np.array(batch_label)\n",
    "            batch_label = batch_label.repeat(batch_num)#标签重复四次，获取真实label\n",
    "            \n",
    "            #shuffle，打乱类别顺序\n",
    "            index = list(range(batch_size))\n",
    "            batch_data = batch_data[index]\n",
    "            batch_label = batch_label[index]\n",
    "            batch_label = np.expand_dims(batch_label, -1)\n",
    "            \n",
    "            yield batch_data, batch_label\n",
    "            \n",
    "            \n",
    "def train_generator2(data, data_label, batch_size=32):\n",
    "    '''\n",
    "    每次从数据样本中抽取4类数据进行训练\n",
    "    :param data: dict, 训练数据\n",
    "    :param batch_size: integer， batch大小，最小为4\n",
    "    :param steps: integer， 单次epoch运行次数\n",
    "    '''\n",
    "    i = 0\n",
    "    while True:        \n",
    "        keys = data.keys()\n",
    "        #while True:\n",
    "        classes = random.sample(keys, 4)#随机选取其中的四类\n",
    "\n",
    "        #求取每类样本的数目\n",
    "        batch_num = int(batch_size / 4)\n",
    "        batch_files = []\n",
    "        batch_label = []#保留label值\n",
    "        for cls in classes:\n",
    "            files = data[cls]#获取当前类别的所有文件\n",
    "            random.shuffle(files)#随机打乱\n",
    "            batch_files = batch_files + files[0:batch_num]#获取每一类数据的batch，组成batch_files\n",
    "            batch_label.append(data_label[cls])#保存所取数据的类别，最终生成的类别list需要再次进行repeat 4操作          \n",
    "\n",
    "        batch_data = load_image(batch_files)\n",
    "        batch_label = np.array(batch_label)\n",
    "        batch_label = batch_label.repeat(batch_num)#标签重复四次，获取真实label\n",
    "\n",
    "        #shuffle，打乱类别顺序\n",
    "        index = list(range(batch_size))\n",
    "        batch_data = batch_data[index]\n",
    "        batch_label = batch_label[index]\n",
    "        batch_label = np.expand_dims(batch_label, -1)\n",
    "\n",
    "        yield batch_data, batch_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 224, 224, 3) [[ 4]\n",
      " [11]\n",
      " [ 0]\n",
      " [18]]\n"
     ]
    }
   ],
   "source": [
    "#check iterator\n",
    "gen = train_generator2(data_train, data_label, batch_size=4)\n",
    "\n",
    "for i in range(1):\n",
    "    batch_data, batch_label = next(gen)\n",
    "    print(batch_data.shape, batch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_gen每次全部进行，不需要进行shuffle\n",
    "#虽然可以直接传入字典，但是从字典中获取片段相比list有些耗时\n",
    "def val_generator(val_files, val_label, batch_size=32):\n",
    "    '''\n",
    "    每次从数据样本中抽取4类数据进行训练\n",
    "    :param data: dict, 训练数据\n",
    "    :param data_label: dict，用于保存数据标签\n",
    "    :param batch_size: integer， batch大小\n",
    "    :param steps: integer， 单次epoch运行次数\n",
    "    '''\n",
    "    while True:\n",
    "        steps = int(len(val_files)//batch_size)\n",
    "        for i in range(steps):               \n",
    "            #求取每类样本的数目\n",
    "            batch_files = val_files[i*batch_size:(i+1)*batch_size]#获取文件切片\n",
    "            #数据加载，并load标签\n",
    "            batch_data = []\n",
    "            batch_label = []\n",
    "            \n",
    "            for file in batch_files:\n",
    "                img = cv2.imread(file)\n",
    "                img =preprocess(img, target_size=224)\n",
    "                batch_data.append(img)\n",
    "                batch_label.append(val_label[file])\n",
    "            \n",
    "            batch_data = np.array(batch_data)\n",
    "            batch_label = np.array(batch_label)\n",
    "            batch_label = np.expand_dims(batch_label, -1)\n",
    "            \n",
    "            yield batch_data, batch_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 224, 224, 3) [[19]\n",
      " [ 4]\n",
      " [16]\n",
      " [ 2]]\n"
     ]
    }
   ],
   "source": [
    "#check val iterator\n",
    "val_gen = val_generator(val_files, data_val, 8)\n",
    "for i in range(1):\n",
    "    batch_data, batch_label = next(gen)\n",
    "    print(batch_data.shape, batch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'diningtable': 0, 'pottedplant': 1, 'horse': 2, 'sheep': 3, 'chair': 4, 'cat': 5, 'dog': 6, 'car': 7, 'motorbike': 8, 'bird': 9, 'bicycle': 10, 'boat': 11, 'bus': 12, 'train': 13, 'cow': 14, 'person': 15, 'aeroplane': 16, 'bottle': 17, 'sofa': 18, 'tvmonitor': 19}\n"
     ]
    }
   ],
   "source": [
    "print(data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./models/resnext50.py')\n",
    "from models.resnext50 import ResNext50\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_weighted(labels):\n",
    "    '''\n",
    "    根据label进行权重初始化\n",
    "    '''\n",
    "    weighted = {}#用list好处理\n",
    "    for label in labels:\n",
    "        weighted[label]=1.0#默认权重都是1\n",
    "    return weighted\n",
    "\n",
    "def weighted_categoritical_loss(weighted):\n",
    "    print('check33333333333')\n",
    "    def weighted_categoritical_loss_inline(y_true, y_pred):\n",
    "        print(\"check111111111111\", y_true.shape)\n",
    "        print('check22222222222222', y_pred.shape)\n",
    "        result2 = -tf.reduce_sum(y_true*tf.log(y_pred),1)\n",
    "        return result2\n",
    "    return weighted_categoritical_loss_inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置weigithed\n",
    "labels = list(range(20))\n",
    "weighted = initial_weighted(labels)\n",
    "weighted[2]=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "#开始check模型\n",
    "import tensorflow as tf\n",
    "\n",
    "train_gen = train_generator2(data_train, data_label, batch_size=4)#当iterator设置100时，会报错，为什么？难道是我之前理解有误？\n",
    "val_gen = val_generator(val_files, data_val, 4)\n",
    "\n",
    "print(len(data_label))\n",
    "#model设置\n",
    "#model = Inception_Resnet_v2(input_shape=(299,299,3), classes = len(data_label))\n",
    "#model = ResNet50(input_shape=(224,224,3), classes = len(data_label))\n",
    "model = ResNext50(input_shape=(224,224,3), classes = len(data_label))\n",
    "model.compile(optimizer=keras.optimizers.SGD(), loss=keras.losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
    "#model.compile(optimizer=keras.optimizers.SGD(), loss=weighted_categoritical_loss(weighted), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 207s 207ms/step - loss: 4.3810 - acc: 0.0755 - val_loss: 3.0337 - val_acc: 0.0681\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 168s 168ms/step - loss: 3.0554 - acc: 0.0917 - val_loss: 4.3192 - val_acc: 0.0858\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 168s 168ms/step - loss: 2.9153 - acc: 0.1158 - val_loss: 3.9018 - val_acc: 0.1202\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 168s 168ms/step - loss: 2.7721 - acc: 0.1480 - val_loss: 2.8355 - val_acc: 0.1148\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 167s 167ms/step - loss: 2.6905 - acc: 0.1515 - val_loss: 4.3715 - val_acc: 0.1172\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 167s 167ms/step - loss: 2.6419 - acc: 0.1600 - val_loss: 3.4355 - val_acc: 0.1413\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 167s 167ms/step - loss: 2.5520 - acc: 0.1935 - val_loss: 4.0623 - val_acc: 0.1379\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 167s 167ms/step - loss: 2.4819 - acc: 0.2230 - val_loss: 9.3966 - val_acc: 0.0759\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 167s 167ms/step - loss: 2.4802 - acc: 0.2155 - val_loss: 4.8542 - val_acc: 0.1190\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 167s 167ms/step - loss: 2.4327 - acc: 0.2250 - val_loss: 4.4597 - val_acc: 0.0917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffa9a6fedd8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator=train_gen,\n",
    "                      steps_per_epoch=1000,\n",
    "                      epochs=10,\n",
    "                      verbose = 1,\n",
    "                      validation_data=val_gen,\n",
    "                      validation_steps = len(val_files)/4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
